{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import ast\n",
    "import gzip\n",
    "import os\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Run this code on the unfiltered CSV that was extracted from the raw data\n",
    "\n",
    "\n",
    "\n",
    "def filter_std_player_classes_monsters(df):\n",
    "    # Convert the list of standard classes to a set for faster membership checks\n",
    "    standard_classes = {\n",
    "        'Barbarian', 'Bard', 'Cleric', 'Druid', 'Fighter', 'Monk', \n",
    "        'Paladin', 'Ranger', 'Rogue', 'Sorcerer', 'Warlock', 'Wizard', 'Blood Hunter'\n",
    "    }\n",
    "\n",
    "    def are_all_classes_standard(player_list_str):\n",
    "        # Convert the string representation of the list only once\n",
    "        player_list = ast.literal_eval(player_list_str)\n",
    "        for player in player_list:\n",
    "            # Iterate through each class information tuple\n",
    "            for class_info in player['class']:\n",
    "                # Check against the set of standard classes\n",
    "                if class_info[0].strip() not in standard_classes:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    # Filter rows based on player_info\n",
    "    tqdm.pandas(desc=\"Filtering Players\")\n",
    "    df_filtered = df[df['player_info'].progress_apply(are_all_classes_standard)]\n",
    "    \n",
    "    # Filter rows based on monsters_info\n",
    "    df_filtered = df_filtered[df_filtered['monsters_info'] != \"[]\"]\n",
    "\n",
    "    # Filter rows based on party total hpratio\n",
    "    df_filtered = df_filtered.dropna(subset=['party_total_hpratio'])\n",
    "    \n",
    "    # Filter rows based on party_total_precombat_hp\n",
    "    df_filtered = df_filtered[df_filtered['party_total_precombat_hp']<1e10]\n",
    "\n",
    "    # Filter rows based on party_total_precombat_hp\n",
    "    df_filtered = df_filtered[df_filtered['party_total_postcombat_hp']<1e10]\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_filtered.shape)\n",
    "\n",
    "\n",
    "filtered = df_filtered.copy()\n",
    "\n",
    "# Dropping party sizes below 10\n",
    "filtered = filtered[filtered['party_size'] < 10]\n",
    "\n",
    "\n",
    "rows_to_drop = []\n",
    "\n",
    "# Dropping levels above 20\n",
    "for index, row in filtered.iterrows():\n",
    "    player_info = ast.literal_eval(row['player_info'])\n",
    "    for d in player_info:\n",
    "        total_lvl = sum(class_lvl[1] for class_lvl in d['class'])\n",
    "        if total_lvl > 20:\n",
    "            rows_to_drop.append(index)\n",
    "            break  # No need to check further dicts in this row\n",
    "\n",
    "filtered = filtered.drop(rows_to_drop)\n",
    "\n",
    "\n",
    "rows_to_drop = []\n",
    "\n",
    "# Droping those with either no hp ratio or hp's above 350\n",
    "for index, row in filtered.iterrows():\n",
    "    player_info = ast.literal_eval(row['player_info'])\n",
    "    for d in player_info:\n",
    "        # Corrected the check for None\n",
    "        if d['hp_ratio'] is None:\n",
    "            rows_to_drop.append(index)  # Appending index for consistency\n",
    "            break  # Exit the loop after finding the condition met\n",
    "        else:\n",
    "            max_health = d['hp_ratio'][1]\n",
    "            if max_health > 350:\n",
    "                rows_to_drop.append(index)\n",
    "                break  # No need to check further dicts in this row\n",
    "\n",
    "\n",
    "rows_to_drop = []\n",
    "\n",
    "# Dropping those either with no AC value(s) or AC's above 38\n",
    "for index, row in filtered.iterrows():\n",
    "    player_info = ast.literal_eval(row['player_info'])\n",
    "    for d in player_info:\n",
    "        # Corrected the check for None\n",
    "        if d['ac'] is None:\n",
    "            rows_to_drop.append(index)  # Appending index for consistency\n",
    "            break  # Exit the loop after finding the condition met\n",
    "        else:\n",
    "            ac = d['ac']\n",
    "            if ac > 38:\n",
    "                rows_to_drop.append(index)\n",
    "                break  # No need to check further dicts in this row\n",
    "\n",
    "\n",
    "filtered = filtered.drop(rows_to_drop)\n",
    "\n",
    "\n",
    "# Dropping who either do not have an ability score or if an ability score is above 22\n",
    "stats = ['strength', 'dexterity', 'constitution', 'intelligence', 'wisdom', 'charisma']\n",
    "\n",
    "rows_to_drop = []\n",
    "for index, row in filtered.iterrows():\n",
    "    player_info = ast.literal_eval(row['player_info'])\n",
    "    for d in player_info:\n",
    "        # Check if 'stats' is None\n",
    "        if d['stats'] is None:\n",
    "            rows_to_drop.append(index)  # Appending index for consistency\n",
    "            break  # Exit the loop after finding the condition met\n",
    "        else:\n",
    "            # Iterate over the items in the 'stats' dictionary\n",
    "            for key, value in d['stats'].items():\n",
    "                if key in stats and value > 22:\n",
    "                    rows_to_drop.append(index)\n",
    "                    break  # Exit the loop after finding a stat greater than 22\n",
    "\n",
    "\n",
    "filtered = filtered.drop(rows_to_drop)\n",
    "\n",
    "filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding player to monster and monster to player ratios \n",
    "filtered['player_monster_ratio'] = filtered['party_size']/filtered['monster_number']\n",
    "\n",
    "filtered['monster_player_ratio'] = filtered['monster_number']/filtered['party_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "filtered['party_total_class_composition'] = filtered['party_total_class_composition'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Get all unique classes\n",
    "all_classes_str = set(class_name.strip() for sublist in filtered['party_total_class_composition'] for class_name in sublist)\n",
    "\n",
    "# Initialize columns for each class with zeros\n",
    "for class_name in all_classes_str:\n",
    "    filtered[class_name] = 0\n",
    "\n",
    "# Fill in the DataFrame with one-hot encoding\n",
    "for index, row in filtered.iterrows():\n",
    "    corrected_class_names = []\n",
    "    for class_name in row['party_total_class_composition']:\n",
    "        # Correct the class name if needed\n",
    "        if class_name == 'Barbarian ':\n",
    "            class_name = 'Barbarian'\n",
    "        corrected_class_names.append(class_name)\n",
    "        filtered.at[index, class_name.strip()] = 1\n",
    "    # Update the row with corrected class names if necessary\n",
    "    filtered.at[index, 'party_total_class_composition'] = corrected_class_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtered.to_csv('filtered_24_4_5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv('C:\\\\Erdos\\\\Project\\\\DnDFireballProject\\\\is\\\\DnDFireballProject\\\\scaled_filtered_24_4_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(x['party_total_hpratio'], x['weighted_monster_level'])\n",
    "x['party_total_hpratio'].corr(x['weighted_monster_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Throwing shit into models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = x.copy()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_to_include = ['party_size','monster_number','monster_total_level','party_total_ac',\n",
    "                      'party_total_prof_bonus',\n",
    "                      'party_total_strength', 'party_total_dexterity',\n",
    "                       'party_total_constitution',\n",
    "                      'party_total_intelligence',\n",
    "                      'party_total_wisdom',\n",
    "                       'party_total_charisma', 'player_monster_ratio','monster_player_ratio', 'Druid','Cleric','Wizard','Rogue','Warlock','Sorcerer',\n",
    "                       'Blood Hunter','Monk','Bard','Barbarian','Fighter','Paladin','Ranger',\n",
    "                      'weighted_monster_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Prepare your data\n",
    "X = df[['monster_total_level']]  # Predictor\n",
    "y = df['party_total_hpratio']  # Target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Always predicting the mean\n",
    "mean_train = np.mean(y_train)\n",
    "\n",
    "# Create an array filled with the mean value that matches the length of the test set\n",
    "mean_predictions = np.full(shape=y_test.shape, fill_value=mean_train)\n",
    "\n",
    "# Evaluating the model\n",
    "mse = mean_squared_error(y_test, mean_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Baseline Model RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Prepare your data\n",
    "X = df['monster_total_level'].values.reshape(-1,1) # Predictor\n",
    "y = df['party_total_hpratio']  # Target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=455)\n",
    "\n",
    "# Train a Linear Regression Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Baseline Model RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Prepare your data\n",
    "X = df['weighted_monster_level'].values.reshape(-1,1)  # Predictor\n",
    "\n",
    "y = df['party_total_hpratio']  # Target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Linear Regression Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Baseline Model RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Defining the target variable\n",
    "target = 'party_total_hpratio'\n",
    "\n",
    "# Preparing the features and target variable\n",
    "X_rf = df[features_to_include].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Creating the RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "rmse\n",
    "plt.scatter(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## PLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X_PLA = df[features_to_include].copy()\n",
    "Y = df[target]\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_PLA, Y, random_state=35)\n",
    "\n",
    "# Initial PLS model fitting with a predefined number of components\n",
    "pls = PLSRegression(n_components=4)\n",
    "pls.fit(X_train, y_train)\n",
    "Y_pred = pls.predict(X_test)\n",
    "\n",
    "# Using KFold for cross-validation to find the optimal number of components\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "max_components = 20\n",
    "scores = []\n",
    "for i in range(1, max_components + 1):\n",
    "    pls = PLSRegression(n_components=i)\n",
    "    # Make sure to use X_train and y_train for cross-validation\n",
    "    score = -cross_val_score(pls, X_train, y_train, cv=kf, scoring='neg_mean_squared_error').mean()\n",
    "    scores.append(score)\n",
    "    \n",
    "optimal_components = np.argmin(scores) + 1\n",
    "\n",
    "# Evaluation metrics for the initial model (consider re-evaluating after selecting the optimal number of components)\n",
    "print(f\"R-squared: {r2_score(y_test, Y_pred)}\")\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, Y_pred)}\")\n",
    "print(f\"Optimal number of components based on CV: {optimal_components}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
